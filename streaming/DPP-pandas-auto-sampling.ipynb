{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**when sampling from k-dpp**\n",
    "\n",
    "We shall sample until one of the following is met:\n",
    "\n",
    "*  Number of samples is complete based on PCA on kernel (choice of k)\n",
    "*  Stopping criterion based on wilcoxon non-parametric test (early stopping). Using library: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html\n",
    "\n",
    "For speed we will demo the Nystroem kernel\n",
    "\n",
    "Based on the following notebook: https://github.com/chappers/Context-driven-constraints-for-gradient-boosted-models/blob/master/autoML/streaming/dpp-groupfs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from dpp import sample_dpp, decompose_kernel, sample_conditional_dpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75436608850453835"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon(np.random.normal(size=(100,)), np.random.normal(size=(100,))).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_regression()\n",
    "pdf = pd.DataFrame(X)\n",
    "pdf.columns = ['c{}'.format(x) for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = pdf[['c{}'.format(x) for x in range(50, 100)]]\n",
    "X2 = pdf[['c{}'.format(x) for x in range(50)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 13]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx for idx, x in enumerate(pdf.columns) if x in ['c0', 'c13']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2397: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=5.0, pvalue=0.47950012218695348)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon([1,2,3,4,5], [3,4,5,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement DPP version that is similar to what is done above\n",
    "\n",
    "\n",
    "sketch of solution\n",
    "------------------\n",
    "\n",
    "DPP requires a known number of parameters to check at each partial fit!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DPPRegressor(SGDRegressor):\n",
    "    def __init__(self, loss=\"squared_loss\", penalty=\"l2\", alpha=0.0001,\n",
    "                 l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
    "                 shuffle=True, verbose=0, epsilon=0.1,\n",
    "                 random_state=None, learning_rate=\"invscaling\", eta0=0.01,\n",
    "                 power_t=0.25, warm_start=False, average=False, n_iter=None,\n",
    "                 intragroup_alpha=0.05, intergroup_thres=None):\n",
    "        super(DPPRegressor, self).__init__(loss=loss, penalty=penalty,\n",
    "                                           alpha=alpha, l1_ratio=l1_ratio,\n",
    "                                           fit_intercept=fit_intercept,\n",
    "                                           max_iter=max_iter, tol=tol,\n",
    "                                           shuffle=shuffle,\n",
    "                                           verbose=verbose,\n",
    "                                           epsilon=epsilon,\n",
    "                                           random_state=random_state,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           eta0=eta0, power_t=power_t,\n",
    "                                           warm_start=warm_start,\n",
    "                                           average=average, n_iter=n_iter)\n",
    "        self.coef_info = {'cols': [], 'coef':[], 'excluded_cols': []}\n",
    "        self.seen_cols = []\n",
    "        self.base_shape = None\n",
    "        self.intragroup_alpha = intragroup_alpha\n",
    "        self.intergroup_thres = intergroup_thres if intergroup_thres is not None else epsilon\n",
    "    \n",
    "    def _dpp_estimate_k(self, L):\n",
    "        \"\"\"\n",
    "        L is the input kernel\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=None)\n",
    "        pca.fit(L)\n",
    "        return np.min(np.argwhere(np.cumsum(pca.explained_variance_ratio_) > \n",
    "                                  (1-self.intragroup_alpha)))\n",
    "        \n",
    "    \n",
    "    def add_column_exclusion(self, cols):\n",
    "        self.coef_info['excluded_cols'] = list(self.coef_info['excluded_cols']) + list(cols)\n",
    "        \n",
    "    def _fit_columns(self, X_, return_x=True, transform_only=False):\n",
    "        \"\"\"\n",
    "        Method filter through \"unselected\" columns. The goal of this \n",
    "        method is to filter any uninformative columns.\n",
    "        \n",
    "        This will be selected based on index only?\n",
    "        \n",
    "        If return_x is false, it will only return the boolean mask.\n",
    "        \"\"\"\n",
    "        X = X_[X_.columns.difference(self.coef_info['excluded_cols'])]\n",
    "        \n",
    "        # order the columns correctly...\n",
    "        col_order = self.coef_info['cols'] + list([x for x in X.columns if x not in self.coef_info['cols']])\n",
    "        X = X[col_order]\n",
    "        return X\n",
    "\n",
    "    def _reg_penalty(self, X):\n",
    "        col_coef = [(col, coef) for col, coef in zip(X.columns.tolist(), self.coef_) if np.abs(coef) >= self.intergroup_thres]\n",
    "        self.coef_info['cols'] = [x for x, _ in col_coef]\n",
    "        self.coef_info['coef'] = [x for _, x in col_coef]\n",
    "        self.coef_info['excluded_cols'] = [x for x in self.seen_cols if x not in self.coef_info['cols']]\n",
    "        self.coef_ = np.array(self.coef_info['coef'])  \n",
    "    \n",
    "    def _dpp_sel(self, X_, y=None):\n",
    "        \"\"\"\n",
    "        DPP only relies on X. \n",
    "        \n",
    "        We will condition the sampling based on:\n",
    "        *  `self.coef_info['cols']`\n",
    "        \"\"\"\n",
    "        X = np.array(X_)\n",
    "        if X.shape[0] < 1000:\n",
    "            feat_dist = rbf_kernel(X.T)\n",
    "        else:\n",
    "            feat_dist = Nystroem().fit_transform(X.T)\n",
    "        k = self._dpp_estimate_k(feat_dist) - len(self.coef_info['cols'])\n",
    "                \n",
    "        if len(self.coef_info['cols']) == 0:\n",
    "            feat_index = sample_dpp(decompose_kernel(feat_dist), k=k)\n",
    "        else:\n",
    "            cols_to_index = [idx for idx, x in enumerate(X_.columns) if x in self.coef_info['cols']]\n",
    "            feat_index = sample_conditional_dpp(feat_dist, cols_to_index, k=k)\n",
    "        \n",
    "        # iterate over feat_index to determine \n",
    "        # information on wilcoxon test\n",
    "        feat_check = []\n",
    "        excl_check = []\n",
    "        for idx1, val1 in enumerate(feat_index):\n",
    "            for idx2, val2 in enumerate(feat_index):\n",
    "                if idx1 == 0:\n",
    "                    feat_check.append(val1)\n",
    "                if idx1 <= idx2 or idx1 in excl_check or idx2 in excl_check:\n",
    "                    continue\n",
    "                \n",
    "                if wilcoxon(X[:, val1], X[:, val2]).pvalue >= self.intragroup_alpha:\n",
    "                    feat_check.append(val1)\n",
    "                    feat_check.append(val2)\n",
    "                    feat_check = list(set(feat_check))\n",
    "                else:\n",
    "                    excl_check.append(val2)\n",
    "        \n",
    "        index_to_col = [col for idx, col in enumerate(X_.columns) if idx in feat_check]\n",
    "        self.coef_info['cols'] = list(set(self.coef_info['cols'] + index_to_col))\n",
    "        col_rem = X_.columns.difference(self.coef_info['cols'])\n",
    "        self.add_column_exclusion(col_rem)        \n",
    "        \n",
    "    def fit(self, X, y, coef_init=None, intercept_init=None,\n",
    "            sample_weight=None):\n",
    "        self.seen_cols = list(set(self.seen_cols + X.columns.tolist()))\n",
    "        \n",
    "        # TODO: add DPP selection\n",
    "        self.coef_info = {'cols': [], 'coef':[], 'excluded_cols': []}\n",
    "        self._dpp_sel(X, y)\n",
    "        X = self._fit_columns(X)\n",
    "        \n",
    "        super(DPPRegressor, self).fit(X, y, coef_init=coef_init, intercept_init=intercept_init,\n",
    "            sample_weight=sample_weight)\n",
    "        self._reg_penalty(X)\n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y, sample_weight=None):\n",
    "        X_ = X.copy()\n",
    "        self.seen_cols = list(set(self.seen_cols + X.columns.tolist()))\n",
    "        X = X[X.columns.difference(self.coef_info['excluded_cols'])]\n",
    "        \n",
    "        # TODO: add DPP selection\n",
    "        self._dpp_sel(X, y)\n",
    "        X = self._fit_columns(X_)\n",
    "        \n",
    "        # now update coefficients\n",
    "        n_samples, n_features = X.shape\n",
    "        coef_list = np.zeros(n_features, dtype=np.float64, order=\"C\")\n",
    "        coef_list[:len(self.coef_info['coef'])] = self.coef_info['coef']\n",
    "        self.coef_ = coef_list.copy()\n",
    "        \n",
    "        super(DPPRegressor, self).partial_fit(X, y, sample_weight=None)  \n",
    "        self._reg_penalty(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._fit_columns(X, transform_only=True)\n",
    "        return super(DPPRegressor, self).predict(X)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPPRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, intergroup_thres=0.1, intragroup_alpha=0.05,\n",
       "       l1_ratio=0.15, learning_rate='invscaling', loss='squared_loss',\n",
       "       max_iter=1000, n_iter=None, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DPPRegressor(max_iter=1000)\n",
    "model.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2397: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2422: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z = (T - mn - correction) / se\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPPRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, intergroup_thres=0.1, intragroup_alpha=0.05,\n",
       "       l1_ratio=0.15, learning_rate='invscaling', loss='squared_loss',\n",
       "       max_iter=1000, n_iter=None, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.partial_fit(pdf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-212.94777128,  320.21082479,  -96.46891294,  -54.78595823,\n",
       "        -36.81428094,   54.87168851,  -93.49542057,  103.95038936,\n",
       "        165.26515227,  -74.73007888, -118.87163358,   96.5536367 ,\n",
       "          5.24539943,  238.41595736,  -29.50419503,   43.55234033,\n",
       "        -30.19621481,    3.04936598,  -16.29435966,  160.92544535,\n",
       "       -222.25043937,  444.40908113,  -22.92154964,  -67.71627381,\n",
       "       -200.55852412,  423.52242524,  173.16823766,   55.64052251,\n",
       "        108.5712764 ,  -72.14247769, -211.28739099, -172.04297086,\n",
       "       -183.43826894,  302.85898519,  401.76914218,    1.4540655 ,\n",
       "       -309.44659618,    4.71206652,  -96.63542729,   13.33204371,\n",
       "       -437.02168181,  211.83758874,  383.41515714,   37.40018305,\n",
       "         69.25207111,  133.20777767,  -44.94898174, -365.76751482,\n",
       "        222.29835922,   47.15305935, -102.24462188, -288.69831597,\n",
       "       -338.95099292, -456.38420546, -224.00298753,  -47.00136985,\n",
       "        186.24862704, -201.76658292, -307.67925744, -168.16064705,\n",
       "        -62.00662367,  -15.59736391, -108.80686014,  156.06077274,\n",
       "        -38.73131688, -189.47197524, -119.40757406, -224.12368889,\n",
       "        -22.73525359,  -33.39666436,  -91.32998679, -179.43203054,\n",
       "         76.5294537 ,  254.04700088, -257.57173384,  185.51376412,\n",
       "        -62.36516036,  343.84056804,  -33.85107535,   30.44453862,\n",
       "       -652.28399542, -190.84077372,  -28.44891239,  158.37548604,\n",
       "         89.67679656,  -52.24597651, -165.14412718,  176.77799259,\n",
       "        109.88053427,   32.47907263, -180.11517172, -102.9903714 ,\n",
       "       -124.38924682, -155.7358662 ,  -83.80730384, -125.16046841,\n",
       "       -154.60743299, -204.02344082,  -98.75551024, -105.20106267])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
