{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**when sampling from k-dpp**\n",
    "\n",
    "We shall sample until one of the following is met:\n",
    "\n",
    "*  Number of samples is complete based on PCA on kernel (choice of k)\n",
    "*  Stopping criterion based on wilcoxon non-parametric test (early stopping). Using library: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html\n",
    "\n",
    "For speed we will demo the Nystroem kernel\n",
    "\n",
    "Based on the following notebook: https://github.com/chappers/Context-driven-constraints-for-gradient-boosted-models/blob/master/autoML/streaming/dpp-groupfs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from dpp import sample_dpp, decompose_kernel, sample_conditional_dpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31538302187180656"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon(np.random.normal(size=(100,)), np.random.normal(size=(100,))).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=50)\n",
    "pdf = pd.DataFrame(X)\n",
    "pdf.columns = ['c{}'.format(x) for x in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['c50' 'c51' 'c52' 'c53' 'c54' 'c55' 'c56' 'c57' 'c58' 'c59' 'c60' 'c61'\\n 'c62' 'c63' 'c64' 'c65' 'c66' 'c67' 'c68' 'c69' 'c70' 'c71' 'c72' 'c73'\\n 'c74' 'c75' 'c76' 'c77' 'c78' 'c79' 'c80' 'c81' 'c82' 'c83' 'c84' 'c85'\\n 'c86' 'c87' 'c88' 'c89' 'c90' 'c91' 'c92' 'c93' 'c94' 'c95' 'c96' 'c97'\\n 'c98' 'c99'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-336de015d5e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chapm\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1959\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chapm\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2000\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2002\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2003\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chapm\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['c50' 'c51' 'c52' 'c53' 'c54' 'c55' 'c56' 'c57' 'c58' 'c59' 'c60' 'c61'\\n 'c62' 'c63' 'c64' 'c65' 'c66' 'c67' 'c68' 'c69' 'c70' 'c71' 'c72' 'c73'\\n 'c74' 'c75' 'c76' 'c77' 'c78' 'c79' 'c80' 'c81' 'c82' 'c83' 'c84' 'c85'\\n 'c86' 'c87' 'c88' 'c89' 'c90' 'c91' 'c92' 'c93' 'c94' 'c95' 'c96' 'c97'\\n 'c98' 'c99'] not in index\""
     ]
    }
   ],
   "source": [
    "X1 = pdf[['c{}'.format(x) for x in range(50, 100)]]\n",
    "X2 = pdf[['c{}'.format(x) for x in range(50)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 13]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx for idx, x in enumerate(pdf.columns) if x in ['c0', 'c13']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import functools\n",
    "a = Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X, axis=0).reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_separability(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the class separability based on the mitra paper    \n",
    "    \"\"\"    \n",
    "    # get prior probs\n",
    "    prior_proba = Counter(y)\n",
    "    \n",
    "    s_w = []    \n",
    "    for class_ in prior_proba.keys():\n",
    "        mask = y==class_\n",
    "        X_sel = X[mask, :]\n",
    "        cov_sig = np.cov(X_sel.T)\n",
    "        s_w.append(cov_sig * prior_proba[class_])\n",
    "    s_w = np.add(*s_w)\n",
    "    \n",
    "    s_b = []\n",
    "    m_o = np.mean(X, axis=0).reshape(-1, 1)\n",
    "    for class_ in prior_proba.keys():\n",
    "        mu_m = prior_proba[class_] - m_o\n",
    "        s_b.append(np.dot(mu_m, mu_m.T))\n",
    "    s_b = np.add(*s_b)\n",
    "    s_b_inv = np.linalg.inv(s_b)\n",
    "    S = np.trace(np.dot(s_b_inv, s_w)) # mitra\n",
    "    S_ = np.trace(s_w)/np.trace(s_b)   # OGFS (criterion1) \n",
    "    # return result for each weight...\n",
    "    S_diag = np.diag(s_w)/np.diag(s_b) # OGFS (criterion2)\n",
    "    return s_w, s_b, S, S_, S_diag        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa, b, ss, ss1, ss2 = class_separability(X, y) # lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-525155697915243.56"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020636408117874133"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0240139 ,  0.02007249,  0.02495617,  0.02661666,  0.00870059,\n",
       "        0.02273024,  0.01961968,  0.01862494,  0.01766042,  0.02102656,\n",
       "        0.02273644,  0.01569287,  0.0322513 ,  0.01712509,  0.02149026,\n",
       "        0.02604032,  0.01953098,  0.02339917,  0.01672218,  0.02250925,\n",
       "        0.02576408,  0.01876138,  0.01963613,  0.01890476,  0.01844898,\n",
       "        0.0177503 ,  0.02238352,  0.02492517,  0.01461841,  0.01847299,\n",
       "        0.02157517,  0.02165669,  0.0178774 ,  0.01832115,  0.01606219,\n",
       "        0.02343537,  0.02064203,  0.024978  ,  0.02166904,  0.02308293,\n",
       "        0.02080895,  0.02528485,  0.02105033,  0.01950124,  0.02492153,\n",
       "        0.01845239,  0.01952898,  0.01927632,  0.01691564,  0.01555949])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    mm = MinMaxScaler()\n",
    "    X_mm = mm.fit_transform(X)\n",
    "    Dpq = euclidean_distances(X_mm)\n",
    "    D_bar = np.mean([x for x in np.triu(Dpq).flatten() if x != 0])\n",
    "    alpha = -np.log(0.5)/D_bar\n",
    "    sim_pq = np.exp(-alpha * Dpq)\n",
    "    log_sim_pq = np.log(sim_pq)\n",
    "    entropy = -2*np.sum(np.triu(sim_pq*log_sim_pq + ((1-sim_pq)*np.log((1-sim_pq))), 1))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wilcoxon_group(X, f):\n",
    "    # X is a matrix, f is a single vector\n",
    "    if len(X.shape) == 1:\n",
    "        return wilcoxon(X, f).pvalue\n",
    "    # now we shall perform and check each one...and return only the lowest pvalue\n",
    "    return np.min([wilcoxon(x, f) for x in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement DPP version that is similar to what is done above\n",
    "\n",
    "\n",
    "sketch of solution\n",
    "------------------\n",
    "\n",
    "DPP requires a known number of parameters to check at each partial fit!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DPPClassifier(SGDClassifier):\n",
    "    def __init__(self, loss=\"log\", penalty='l2', alpha=0.0001, l1_ratio=0.15,\n",
    "                 fit_intercept=True, max_iter=None, tol=None, shuffle=True,\n",
    "                 verbose=0, epsilon=0.1, n_jobs=1,\n",
    "                 random_state=None, learning_rate=\"optimal\", eta0=0.0,\n",
    "                 power_t=0.5, class_weight=None, warm_start=False,\n",
    "                 average=False, n_iter=None, \n",
    "                 intragroup_alpha=0.05, intergroup_thres=None):\n",
    "        super(DPPClassifier, self).__init__(\n",
    "            loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio,\n",
    "            fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,\n",
    "            shuffle=shuffle, verbose=verbose, epsilon=epsilon, n_jobs=n_jobs,\n",
    "            random_state=random_state, learning_rate=learning_rate, eta0=eta0,\n",
    "            power_t=power_t, class_weight=class_weight, warm_start=warm_start,\n",
    "            average=average, n_iter=n_iter)\n",
    "        self.coef_info = {'cols': [], 'coef':[], 'excluded_cols': []}\n",
    "        self.seen_cols = []\n",
    "        self.base_shape = None\n",
    "        self.intragroup_alpha = intragroup_alpha\n",
    "        self.intergroup_thres = intergroup_thres if intergroup_thres is not None else epsilon\n",
    "    \n",
    "    def _dpp_estimate_k(self, L):\n",
    "        \"\"\"\n",
    "        L is the input kernel\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=None)\n",
    "        pca.fit(L)\n",
    "        return np.min(np.argwhere(np.cumsum(pca.explained_variance_ratio_) > \n",
    "                                  (1-self.intragroup_alpha)))\n",
    "        \n",
    "    \n",
    "    def add_column_exclusion(self, cols):\n",
    "        self.coef_info['excluded_cols'] = list(self.coef_info['excluded_cols']) + list(cols)\n",
    "        \n",
    "    def _fit_columns(self, X_, return_x=True, transform_only=False):\n",
    "        \"\"\"\n",
    "        Method filter through \"unselected\" columns. The goal of this \n",
    "        method is to filter any uninformative columns.\n",
    "        \n",
    "        This will be selected based on index only?\n",
    "        \n",
    "        If return_x is false, it will only return the boolean mask.\n",
    "        \"\"\"\n",
    "        X = X_[X_.columns.difference(self.coef_info['excluded_cols'])]\n",
    "        \n",
    "        # order the columns correctly...\n",
    "        col_order = self.coef_info['cols'] + list([x for x in X.columns if x not in self.coef_info['cols']])\n",
    "        X = X[col_order]\n",
    "        return X\n",
    "\n",
    "    def _reg_penalty(self, X):\n",
    "        col_coef = [(col, coef) for col, coef in zip(X.columns.tolist(), self.coef_.flatten()) if np.abs(coef) >= self.intergroup_thres]\n",
    "        self.coef_info['cols'] = [x for x, _ in col_coef]\n",
    "        self.coef_info['coef'] = [x for _, x in col_coef]\n",
    "        self.coef_info['excluded_cols'] = [x for x in self.seen_cols if x not in self.coef_info['cols']]\n",
    "        self.coef_ = np.array(self.coef_info['coef']).reshape(1, -1) \n",
    "    \n",
    "    def _dpp_sel(self, X_, y=None):\n",
    "        \"\"\"\n",
    "        DPP only relies on X. \n",
    "        \n",
    "        We will condition the sampling based on:\n",
    "        *  `self.coef_info['cols']`\n",
    "        \n",
    "        After sampling it will go ahead and then perform grouped wilcoxon selection.\n",
    "        \"\"\"\n",
    "        X = np.array(X_)\n",
    "        if X.shape[0] < 1000:\n",
    "            feat_dist = rbf_kernel(X.T)\n",
    "        else:\n",
    "            feat_dist = Nystroem().fit_transform(X.T)\n",
    "        k = self._dpp_estimate_k(feat_dist) - len(self.coef_info['cols'])\n",
    "                \n",
    "        if len(self.coef_info['cols']) == 0:\n",
    "            feat_index = sample_dpp(decompose_kernel(feat_dist), k=k)\n",
    "        else:\n",
    "            cols_to_index = [idx for idx, x in enumerate(X_.columns) if x in self.coef_info['cols']]\n",
    "            feat_index = sample_conditional_dpp(feat_dist, cols_to_index, k=k)\n",
    "        \n",
    "        # select features using entropy measure\n",
    "        # how can we order features from most to least relevant first?\n",
    "        # we chould do it using f test? Or otherwise - presume DPP selects best one first\n",
    "        \"\"\"\n",
    "        feat_entropy = []\n",
    "        excl_entropy = []\n",
    "        X_sel = X[:, feat_index]\n",
    "        \n",
    "        for idx, feat in enumerate(X_sel.T):\n",
    "            if len(feat_entropy) == 0:\n",
    "                feat_entropy.append(idx)\n",
    "                continue\n",
    "            if entropy(X_sel[:, feat_entropy]) > entropy(X_sel[:, feat_entropy+[idx]]):\n",
    "                feat_entropy.append(idx)\n",
    "            else:\n",
    "                excl_entropy.append(idx)\n",
    "        \"\"\"\n",
    "        # iterate over feat_index to determine \n",
    "        # information on wilcoxon test\n",
    "        # as the feat index are already \"ordered\" as that is how DPP would\n",
    "        # perform the sampling - we will do the single pass in the same\n",
    "        # way it was approached in the OGFS\n",
    "        feat_check = []\n",
    "        excl_check = []\n",
    "        X_sel = X[:, feat_index]\n",
    "        \n",
    "        for idx, feat in enumerate(X_sel.T):\n",
    "            if len(feat_check) == 0:\n",
    "                feat_check.append(idx)\n",
    "                continue\n",
    "            if wilcoxon_group(X_sel[:, feat_check], feat) >= self.intragroup_alpha:\n",
    "                feat_check.append(idx)\n",
    "            else:\n",
    "                excl_check.append(idx)\n",
    "        index_to_col = [col for idx, col in enumerate(X_.columns) if idx in feat_check]\n",
    "        self.coef_info['cols'] = list(set(self.coef_info['cols'] + index_to_col))\n",
    "        col_rem = X_.columns.difference(self.coef_info['cols'])\n",
    "        self.add_column_exclusion(col_rem)        \n",
    "        \n",
    "    def fit(self, X, y, coef_init=None, intercept_init=None,\n",
    "            sample_weight=None):\n",
    "        self.seen_cols = list(set(self.seen_cols + X.columns.tolist()))\n",
    "        \n",
    "        # TODO: add DPP selection\n",
    "        self.coef_info = {'cols': [], 'coef':[], 'excluded_cols': []}\n",
    "        self._dpp_sel(X, y)\n",
    "        X = self._fit_columns(X)\n",
    "        \n",
    "        super(DPPClassifier, self).fit(X, y, coef_init=coef_init, intercept_init=intercept_init,\n",
    "            sample_weight=sample_weight)\n",
    "        self._reg_penalty(X)\n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y, sample_weight=None):\n",
    "        X_ = X.copy()\n",
    "        self.seen_cols = list(set(self.seen_cols + X.columns.tolist()))\n",
    "        X = X[X.columns.difference(self.coef_info['excluded_cols'])]\n",
    "        \n",
    "        # TODO: add DPP selection\n",
    "        self._dpp_sel(X, y)\n",
    "        X = self._fit_columns(X_)\n",
    "        \n",
    "        # now update coefficients\n",
    "        n_samples, n_features = X.shape\n",
    "        coef_list = np.zeros(n_features, dtype=np.float64, order=\"C\")\n",
    "        coef_list[:len(self.coef_info['coef'])] = self.coef_info['coef']\n",
    "        self.coef_ = np.array(coef_list).reshape(1, -1)\n",
    "        \n",
    "        super(DPPClassifier, self).partial_fit(X, y, sample_weight=None)  \n",
    "        self._reg_penalty(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._fit_columns(X, transform_only=True)\n",
    "        return super(DPPClassifier, self).predict(X)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPPClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, intergroup_thres=0.1,\n",
       "       intragroup_alpha=0.05, l1_ratio=0.15, learning_rate='optimal',\n",
       "       loss='log', max_iter=1000, n_iter=None, n_jobs=1, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DPPClassifier(max_iter=1000)\n",
    "model.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2397: UserWarning: Warning: sample size too small for normal approximation.\n",
      "  warnings.warn(\"Warning: sample size too small for normal approximation.\")\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:2422: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z = (T - mn - correction) / se\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPPClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, intergroup_thres=0.1,\n",
       "       intragroup_alpha=0.05, l1_ratio=0.15, learning_rate='optimal',\n",
       "       loss='log', max_iter=1000, n_iter=None, n_jobs=1, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.partial_fit(pdf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
